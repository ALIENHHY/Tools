{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 读取 JSON 格式的知识库\n",
    "json_path = \"rag-test-KB.json\"\n",
    "with open(json_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. **创建 Document 对象**\n",
    "documents = []\n",
    "for item in data:\n",
    "    knowledge = item[\"knowledge\"]\n",
    "    doc = Document(page_content=knowledge, metadata={\"source\": item[\"knowledge\"]})\n",
    "    documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11032\\AppData\\Local\\Temp\\ipykernel_29988\\3068361344.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedder = HuggingFaceEmbeddings(model_name=\"./all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "# 3. **使用 HuggingFace 本地嵌入模型**\n",
    "embedder = HuggingFaceEmbeddings(model_name=\"./all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. **创建 FAISS 向量数据库**\n",
    "vector_store = FAISS.from_documents(documents, embedder)\n",
    "retriever = vector_store.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11032\\AppData\\Local\\Temp\\ipykernel_29988\\3372562694.py:2: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(\n"
     ]
    }
   ],
   "source": [
    "# 5. **使用 DeepSeek 1.5B 作为 LLM**\n",
    "llm = Ollama(\n",
    "    model=\"deepseek-r1:1.5b\",\n",
    "    system=\"你是一个严格的 AI 助手，你只能基于提供的上下文回答问题。\\n\"\n",
    "           \"如果你无法从上下文中找到答案，请回答 '我不知道'。\\n\"\n",
    "           \"不要编造信息。\\n\"\n",
    "           \"你必须引用相关内容，并用 `【来源: source】` 标明出处。\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. **定义 QA Prompt（适用于长知识）**\n",
    "QA_PROMPT = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    你是一个专业的知识助手，你只能基于提供的上下文回答问题。\\n\n",
    "    请确保你的回答只包含来自上下文的信息，不要编造内容。\\n\n",
    "    你必须引用相关内容，并用 `【来源: source】` 标明出处。\\n\n",
    "    相关知识:\n",
    "    {context}\n",
    "    {question}\n",
    "    答案:\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11032\\AppData\\Local\\Temp\\ipykernel_29988\\3571466328.py:2: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llm_chain = LLMChain(llm=llm, prompt=QA_PROMPT, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "# 7. **创建 LLMChain**\n",
    "llm_chain = LLMChain(llm=llm, prompt=QA_PROMPT, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11032\\AppData\\Local\\Temp\\ipykernel_29988\\2683357691.py:2: LangChainDeprecationWarning: This class is deprecated. Use the `create_stuff_documents_chain` constructor instead. See migration guide here: https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain/\n",
      "  combine_documents_chain = StuffDocumentsChain(\n"
     ]
    }
   ],
   "source": [
    "# 8. **文档处理链**\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=llm_chain,\n",
    "    document_variable_name=\"context\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\11032\\AppData\\Local\\Temp\\ipykernel_29988\\2122260647.py:2: LangChainDeprecationWarning: This class is deprecated. Use the `create_retrieval_chain` constructor instead. See migration guide here: https://python.langchain.com/docs/versions/migrating_chains/retrieval_qa/\n",
      "  qa = RetrievalQA(\n"
     ]
    }
   ],
   "source": [
    "# 9. **创建 RetrievalQA**\n",
    "qa = RetrievalQA(\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. **LLM 自检 Prompt（防止幻觉）**\n",
    "CHECK_PROMPT = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    你刚刚生成了以下答案:\n",
    "    \"{answer}\"\n",
    "    请检查这个答案是否严格基于以下上下文:\n",
    "    {context}\n",
    "    如果答案和上下文不匹配，请回答：\"这个答案可能包含幻觉\"。\n",
    "    如果答案完全来自上下文，请回答：\"这个答案是基于上下文的\"。\n",
    "    \"\"\"\n",
    ")\n",
    "llm_chain_check = LLMChain(llm=llm, prompt=CHECK_PROMPT, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. **用户输入查询**\n",
    "query = \"国足能进世界杯吗\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. **检索相关文档**\n",
    "search_results = retriever.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ 该回答是基于上下文的。\n",
      "\n",
      "最终回答: <think>\n",
      "好的，我现在要回答用户的问题“国家足能否进世界杯”。首先，我需要判断是否有相关法规或标准支持这一点。根据之前的对话和知识库内容，我记得有关于无菌药品生产标准中提到的自净时间。不过，关于足球比赛的规则，特别是关于是否可以参赛进入世界杯 tournament，这应该是国际足联的内部政策。\n",
      "\n",
      "根据我的信息，国际足联规定，球队必须通过资格审查，并且只能代表国家参加世界杯 tournament。因此，如果国家足没有资格入选，就无法进世界杯。这一点是明确的，所以我的回答应该基于这一官方立场。\n",
      "</think>\n",
      "\n",
      "国家足不能进入世界杯 tournament，因为国际足联将严格筛选球队资格并Only allow qualifying teams to participate.\n"
     ]
    }
   ],
   "source": [
    "# 13. **检查相似度，避免误答**\n",
    "similarity_threshold = 0.5\n",
    "if not search_results or min([doc.metadata.get(\"score\", 1) for doc in search_results]) < similarity_threshold:\n",
    "    print(\"\\n回答: 我不知道\")\n",
    "else:\n",
    "    # 14. **生成回答**\n",
    "    response = qa(query)\n",
    "    # 15. **让 LLM 自检是否有幻觉**\n",
    "    context_text = \"\\n\\n\".join([doc.page_content for doc in search_results])\n",
    "    check_response = llm_chain_check.run({\"answer\": response[\"result\"], \"context\": context_text})\n",
    "    if \"幻觉\" in check_response:\n",
    "        print(\"\\n⚠️ 该回答可能包含幻觉，建议用户谨慎使用！\")\n",
    "    else:\n",
    "        print(\"\\n✅ 该回答是基于上下文的。\")\n",
    "    print(\"\\n最终回答:\", response[\"result\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
